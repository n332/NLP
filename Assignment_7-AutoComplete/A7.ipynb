{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load and Preprocess Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"en_US.twitter.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "\n",
    "    sentences = data.split ('\\n')\n",
    "\n",
    "    # drop leading and trailing spaces\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    # drop sentences if they are empty strings\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences (sentences):\n",
    "\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        sentence = sentence.lower()\n",
    "        tokenised = nltk.word_tokenize(sentence)\n",
    "\n",
    "        tokenized_sentences.append(tokenised)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data (data):\n",
    "\n",
    "    sentences = split_to_sentences(data)\n",
    "\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(data)\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words (tokenize_sentences):\n",
    "\n",
    "    word_counts = {}\n",
    "\n",
    "    for sentence in tokenize_sentences:\n",
    "\n",
    "        for token in sentence:\n",
    "\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling 'Out of Vocabulary' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency (tokenize_sentences, count_threshold):\n",
    "\n",
    "    closed_vocab = []\n",
    "\n",
    "    word_counts = count_words(tokenize_sentences)\n",
    "\n",
    "    for word, cnt in word_counts.items():\n",
    "\n",
    "        if cnt >= count_threshold:\n",
    "\n",
    "            closed_vocab.append(word)\n",
    "    \n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "\n",
    "    vocabulary = set(vocabulary)\n",
    "\n",
    "    replaced_tokenized_sentences = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "\n",
    "        replaced_sentence = []\n",
    "\n",
    "        for token in sentence:\n",
    "\n",
    "            if token in vocabulary:\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "\n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data (train_data, test_data, count_threshold):\n",
    "\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary)\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary)\n",
    "\n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data, minimum_freq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Develop n-gram based language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "\n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in data: \n",
    "        \n",
    "        sentence = [start_token] * n+ sentence + [end_token]\n",
    "\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        for i in range(len(sentence) if n==1 else len(sentence)-1): \n",
    "        \n",
    "            n_gram = sentence[i:i+n]\n",
    "            \n",
    "            if n_gram in n_grams.keys(): \n",
    "            \n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    if previous_n_gram in n_gram_counts:\n",
    "        previous_n_gram_count = n_gram_counts[previous_n_gram]\n",
    "    else:\n",
    "        previous_n_gram_count = 0\n",
    "\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "    if n_plus1_gram in n_plus1_gram_counts:\n",
    "        n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram]\n",
    "    else:\n",
    "        n_plus1_gram_count = 0\n",
    "\n",
    "    numerator = n_plus1_gram_count + k\n",
    "\n",
    "    probability = numerator / denominator\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate probabilities for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "\n",
    "    previous_n_gram  = tuple(previous_n_gram)\n",
    "\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    probabilities = {}\n",
    "\n",
    "    for word in vocabulary:\n",
    "\n",
    "        prob = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)        \n",
    "        probabilities[word] = prob\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count and probability matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix (n_plus1_gram_counts, vocabulary):\n",
    "\n",
    "    vocabulary = vocabulary + [\"<e>\", \"unk\"]\n",
    "\n",
    "    n_grams = []\n",
    "\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    \n",
    "    n_grams = list(set(n_grams))\n",
    "\n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
    "\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
    "\n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "\n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    sentence = tuple(sentence)\n",
    "    N = len(sentence)\n",
    "    \n",
    "    product_pi = 1.0\n",
    "\n",
    "    for t in range (n, N):\n",
    "\n",
    "        n_gram = sentence[t-n:t]\n",
    "\n",
    "        word = sentence[t]\n",
    "\n",
    "        probability = estimate_probability(word,n_gram, n_gram_counts, n_plus1_gram_counts, len(unique_words), k=1)\n",
    "\n",
    "        product_pi *= 1/ probability\n",
    "\n",
    "    perplexity = product_pi ** (1/float(N))\n",
    "\n",
    "    return perplexity\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Build an auto-complete system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    \n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "\n",
    "    for word, prob in probabilities.items():\n",
    "\n",
    "        if start_with != None:\n",
    "\n",
    "            if not word.startswith(start_with):\n",
    "                continue\n",
    "        \n",
    "        if prob > max_prob:\n",
    "\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 5 : Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like',\n",
      "\tand the suggested word is `a` with a probability of 0.2727\n",
      "\n",
      "The previous words are 'i like', the suggestion must start with `c`\n",
      "\tand the suggested word is `cat` with a probability of 0.0909\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "# test your code when setting the starts_with\n",
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8870da4d7f4078c6c74dd4faac4c9b512f8b094e6c40f9750f01327815903b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
