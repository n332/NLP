{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p431Kuu6Mq_",
        "outputId": "7beb90c1-3c81-49da-eee7-b0c3dbe2465d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Nour\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import wikipedia\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import csv\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaKEpGMO6MrG"
      },
      "source": [
        "# Collecting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgS4nXeu6MrJ"
      },
      "outputs": [],
      "source": [
        "Training_Data_Animals = ''\n",
        "Test__Data_Animals = ''\n",
        "Training_Data_Arts = ''\n",
        "Test__Data_Arts = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShtGmOoS6MrK"
      },
      "outputs": [],
      "source": [
        "duck = wikipedia.page(\"Ducks Animals\")\n",
        "cat = wikipedia.page(\"Cats Animals\")\n",
        "dog = wikipedia.page(\"Dogs Animals\")\n",
        "mouse = wikipedia.page(\"Mouse Animals\")\n",
        "turtle = wikipedia.page(\"Turtles Animals\")\n",
        "parrot = wikipedia.page(\"Parrot Animals\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n15KhjS36MrL",
        "outputId": "181cfe71-a588-48ec-da83-943bb77c7a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n"
          ]
        }
      ],
      "source": [
        "if (len(duck.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Training_Data_Animals = Training_Data_Animals + duck.content\n",
        "\n",
        "if (len(cat.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Training_Data_Animals = Training_Data_Animals + cat.content\n",
        "\n",
        "if (len(dog.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Training_Data_Animals = Training_Data_Animals + dog.content\n",
        "\n",
        "if (len(mouse.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Training_Data_Animals = Training_Data_Animals + mouse.content\n",
        "\n",
        "if (len(turtle.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Training_Data_Animals = Training_Data_Animals + turtle.content\n",
        "\n",
        "if (len(parrot.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Test__Data_Animals = Test__Data_Animals + parrot.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMFiiUsP6MrM"
      },
      "outputs": [],
      "source": [
        "Art1 = wikipedia.page(\"Ceramic art\")\n",
        "Art2 = wikipedia.page(\"Conceptual art\")\n",
        "Art3 = wikipedia.page(\"Drawing\")\n",
        "Art4 = wikipedia.page(\"Photography\")\n",
        "Art5 = wikipedia.page(\"Sculpture art\")\n",
        "Art6 = wikipedia.page(\"Performing arts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLmxRnpd6MrO",
        "outputId": "dd6fd1fc-4628-4ecd-fdcf-3e18700672b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n"
          ]
        }
      ],
      "source": [
        "if (len(Art1.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Training_Data_Arts = Training_Data_Arts + Art1.content\n",
        "\n",
        "if (len(Art2.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Training_Data_Arts = Training_Data_Arts + Art2.content\n",
        "\n",
        "if (len(Art3.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Training_Data_Arts = Training_Data_Arts + Art3.content\n",
        "\n",
        "if (len(Art4.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Training_Data_Arts = Training_Data_Arts + Art4.content\n",
        "\n",
        "if (len(Art5.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Training_Data_Arts = Training_Data_Arts + Art5.content\n",
        "\n",
        "if (len(Art6.content.split()) > 100):\n",
        "    print(\"ok\")\n",
        "    Test__Data_Arts = Test__Data_Arts + Art6.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKYuBXK76MrP"
      },
      "outputs": [],
      "source": [
        "Training_Data_Animals = re.split('[.]', Training_Data_Animals)\n",
        "Training_Data_Arts = re.split('[.]', Training_Data_Arts)\n",
        "Test__Data_Animals = re.split('[.]', Test__Data_Animals)\n",
        "Test__Data_Arts = re.split('[.]', Test__Data_Arts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2HCblOr6MrQ"
      },
      "outputs": [],
      "source": [
        "Train_X = Training_Data_Animals + Training_Data_Arts        \n",
        "Test_X = Test__Data_Animals + Test__Data_Arts               \n",
        "\n",
        "\n",
        "# animals ->  0        Arts ->  1\n",
        "\n",
        "train_y = np.append(np.zeros(len(Training_Data_Animals)), np.ones(len(Training_Data_Arts)))\n",
        "test_y = np.append(np.zeros(len(Test__Data_Animals)), np.ones(len(Test__Data_Arts)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHzssjIi6MrR"
      },
      "outputs": [],
      "source": [
        "header = ['sentences','lable' ]\n",
        "\n",
        "with open('train.csv', 'w', encoding='UTF8', newline='') as f:\n",
        "\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    writer.writerow(header)\n",
        "\n",
        "        \n",
        "    for i in range (len (Train_X)):\n",
        "\n",
        "        data = [Train_X[i],int(train_y[i])]\n",
        "\n",
        "\n",
        "        writer.writerow(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUqHwFYj6MrS"
      },
      "outputs": [],
      "source": [
        "header = ['sentences','lable' ]\n",
        "\n",
        "with open('test.csv', 'w', encoding='UTF8', newline='') as f:\n",
        "\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    writer.writerow(header)\n",
        "\n",
        "        \n",
        "    for i in range (len (Test_X)):\n",
        "\n",
        "        data = [Test_X[i],int(test_y[i])]\n",
        "\n",
        "\n",
        "        writer.writerow(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLk2GnRz6MrT"
      },
      "source": [
        "# 0. Import The data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gC4Sa_C6MrU"
      },
      "outputs": [],
      "source": [
        "training_raw_Data = pd.read_csv(\"train.csv\")\n",
        "Test_raw_Data = pd.read_csv(\"test.csv\")\n",
        "\n",
        "training_raw_Data = training_raw_Data.dropna()\n",
        "Test_raw_Data = Test_raw_Data.dropna()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIyZtC6C6MrV"
      },
      "source": [
        "# 1. The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGJwWxx96MrY"
      },
      "source": [
        "## 1.1. get the data and the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqsrVgdv6MrZ"
      },
      "outputs": [],
      "source": [
        "X_train = training_raw_Data[\"sentences\"].to_numpy().reshape(-1,1)\n",
        "X_test = Test_raw_Data[\"sentences\"].to_numpy().reshape(-1,1)\n",
        "\n",
        "y_train = training_raw_Data[\"lable\"].to_numpy().reshape(-1,1)\n",
        "y_test = Test_raw_Data[\"lable\"].to_numpy().reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg4MxiB66MrZ",
        "outputId": "434505b6-9c80-4ab5-d764-9a623e0d6407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2595, 1)\n",
            "(294, 1)\n",
            "(2595, 1)\n",
            "(294, 1)\n"
          ]
        }
      ],
      "source": [
        "# Check the shape\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlOewft86Mra"
      },
      "source": [
        "## 1.2 Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2_ipCac6Mrb"
      },
      "outputs": [],
      "source": [
        "def preprocessing (doc):\n",
        "\n",
        "    # remove punctuation\n",
        "\n",
        "    clean_doc = \"\"\n",
        "    \n",
        "    punctuation = \"!#$%&()*+,-./:;<=>?@[\\]^_`{|}~0123456789\" \n",
        "    punctuation = punctuation + chr(34) + chr(39)\n",
        "\n",
        "\n",
        "    \n",
        "    for i in range (len(doc)):\n",
        "        if not (doc[i] in punctuation):\n",
        "            clean_doc = clean_doc + doc[i]\n",
        "\n",
        "\n",
        "    # convert to lowercase\n",
        "\n",
        "    clean_doc = clean_doc.lower()\n",
        "\n",
        "    # Note: The rest of the preprocessing will be after tokenization step\n",
        "\n",
        "    return clean_doc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TqQoME06Mrc"
      },
      "source": [
        "## 1.3. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2kb08w86Mrd"
      },
      "outputs": [],
      "source": [
        "def get_tokens(doc):\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    tokens = []\n",
        "\n",
        "    words = doc.split()\n",
        "\n",
        "\n",
        "\n",
        "    for word in words:\n",
        "        word = stemmer.stem(word)\n",
        "        if not word in tokens:\n",
        "            if not word in stop_words:\n",
        "                tokens.append(word)\n",
        "\n",
        "    return tokens\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajpisInU6Mrd"
      },
      "source": [
        "# 2. The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TysKJlkv6Mre"
      },
      "source": [
        "## 2.1. Calculte class prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00JhSLuc6Mrf"
      },
      "outputs": [],
      "source": [
        "def class_prob (doc_label):\n",
        "\n",
        "    Animal_Count = 0\n",
        "    Art_count = 0\n",
        "    m = doc_label.shape[0]\n",
        "\n",
        "    for i in range (m):\n",
        "        if (doc_label[i][0] == 0):\n",
        "            Animal_Count = Animal_Count + 1\n",
        "        else:\n",
        "            Art_count = Art_count + 1\n",
        "\n",
        "    return (Animal_Count/m , Art_count/m)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_A89TBz6Mrg"
      },
      "source": [
        "## 2.2. Clculate conditional Prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOKR-E6I6Mrg"
      },
      "source": [
        "### 2.2.1. Build Word Dictionary *{(\"word\",class):count}*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxleZTBx6Mrh"
      },
      "outputs": [],
      "source": [
        "def build_dict (X_train,y_train):\n",
        "\n",
        "    word_dict = {}\n",
        "\n",
        "    for i in range(X_train.shape[0]):\n",
        "        \n",
        "        sample = preprocessing(X_train[i][0])\n",
        "        tokens = get_tokens(sample)\n",
        "\n",
        "        for token in tokens:\n",
        "            key = (token,y_train[i][0])\n",
        "            \n",
        "            if key in word_dict:\n",
        "                word_dict[key] += 1\n",
        "            else:\n",
        "                word_dict[key] = 1\n",
        "\n",
        "    return word_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgtbRWkO6Mri"
      },
      "outputs": [],
      "source": [
        "word_dict =build_dict (X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Q1EVHm6Mri"
      },
      "source": [
        "### 2.2.2. Calculate every word prob "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxgtiuAl6Mrj"
      },
      "outputs": [],
      "source": [
        "def conditional_prob (word_dict):\n",
        "\n",
        "    word_prob_dict = {}\n",
        "\n",
        "    Animal_count = 0\n",
        "    Art_count = 0\n",
        "\n",
        "    k = len(word_dict)\n",
        "\n",
        "    for key, _ in word_dict.items():\n",
        "        if (key[1] == 0):\n",
        "            Animal_count = Animal_count + 1\n",
        "        else:\n",
        "            Art_count = Art_count + 1\n",
        "\n",
        "    for key, values in word_dict.items():\n",
        "\n",
        "        pair = (key[1],key[0])\n",
        "\n",
        "        if (key[1] == 0):\n",
        "            word_prob_dict[pair] = (values + 1) / (Animal_count + k)\n",
        "        else:\n",
        "            word_prob_dict[pair] = (values + 1) / (Art_count + k)\n",
        "\n",
        "    return word_prob_dict\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUdve3596Mrk"
      },
      "outputs": [],
      "source": [
        "word_prob_dict = conditional_prob (word_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbcHNw0_6Mrk"
      },
      "source": [
        "*saving on hard disk and import*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9Mi5b5R6Mrl",
        "outputId": "3f8a8226-b7d8-46f3-9cde-09b01a4faa5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dictionary saved successfully to file\n"
          ]
        }
      ],
      "source": [
        "with open('probs.pkl', 'wb') as fp:\n",
        "    pickle.dump(word_prob_dict, fp)\n",
        "    print('dictionary saved successfully to file')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9OkQt1l6Mrm"
      },
      "source": [
        "## 2.3. predict "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5l4frgH6Mrm"
      },
      "outputs": [],
      "source": [
        "def predict (test_doc):\n",
        "\n",
        "    with open('probs.pkl', 'rb') as fp:\n",
        "        word_prob_dict = pickle.load(fp)\n",
        "\n",
        "    prob_Animal_doc = 1\n",
        "    prob_Art_doc = 1\n",
        "    prob_animal_word = 1\n",
        "    prob_Art_word = 1\n",
        "    Animal_prop, Art_prop =class_prob (y_train)\n",
        "\n",
        "    tokens = get_tokens(preprocessing(test_doc))\n",
        "\n",
        "    for token in tokens:\n",
        "\n",
        "        Animal_pair = (0,token)\n",
        "        Art_pair = (1,token)\n",
        "\n",
        "        if Animal_pair in word_prob_dict:\n",
        "            prob_animal_word = word_prob_dict[Animal_pair]\n",
        "            prob_Animal_doc = prob_Animal_doc * prob_animal_word\n",
        "\n",
        "        elif Art_pair in word_prob_dict:\n",
        "            prob_Art_word = word_prob_dict[Art_pair]\n",
        "            prob_Art_doc = prob_Art_doc * prob_Art_word\n",
        "\n",
        "    prob_Animal_given_doc = prob_Animal_doc * Animal_prop\n",
        "    prob_Art_given_doc = prob_Art_doc * Art_prop\n",
        "\n",
        "    ratio = prob_Animal_given_doc / prob_Art_given_doc\n",
        "\n",
        "    if ratio > 1:\n",
        "        y_hat = 0\n",
        "    elif ratio < 1:\n",
        "        y_hat = 1\n",
        "\n",
        "    return y_hat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDElX0w96Mrn"
      },
      "source": [
        "# 3. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrB0Br8z6Mro"
      },
      "outputs": [],
      "source": [
        "def calculate_acc (x_test, y_test):\n",
        "\n",
        "    number_of_samples = x_test.shape[0]\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(number_of_samples):\n",
        "        y_hat = predict (x_test[i][0])\n",
        "        if (y_hat == y_test[i][0]):\n",
        "            correct = correct + 1\n",
        "\n",
        "    return correct/number_of_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjvNOdDy6Mrp",
        "outputId": "aa0ef3a0-f58d-45a8-9909-e27e697c5d4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.608843537414966"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_acc (X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAMbN5K46Mrq"
      },
      "source": [
        "Note: I Think the accuracy in not good because the words in class 1 < words in class 2 after preprocessing"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}